{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minor-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run luigiid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "\n",
    "import luigi\n",
    "import pickle\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from column_selector import ColumnSelector\n",
    "# utils функции для датасета\n",
    "from utils import reduce_mem_usage, merge_dfs, features_types, prepare_df_train, prepare_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expired-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline(luigi.Task):\n",
    "    train_csv = luigi.Parameter()\n",
    "    test_csv = luigi.Parameter()\n",
    "    features_csv = luigi.Parameter()\n",
    "    model_file = luigi.Parameter()\n",
    "    \n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget('result.csv')\n",
    " \n",
    "    def run(self):\n",
    "        df_train = pd.read_csv(self.train_csv)\n",
    "        df_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "        df_test = pd.read_csv(self.test_csv, encoding='utf8', sep=',')\n",
    "        df_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        \n",
    "        data = pd.read_csv(self.features_csv, sep='\\t', index_col=[0])\n",
    "        print(\"Датасеты импортированы\")\n",
    "        \n",
    "        df_train = reduce_mem_usage(df_train)\n",
    "        df_test = reduce_mem_usage(df_test)\n",
    "        \n",
    "        data = data.loc[(data['id'].isin(df_train['id'].values)) | ((data['id'].isin(df_test['id'].values)))]\n",
    "        data = reduce_mem_usage(data)\n",
    "        print(\"Датасеты оптимизированы\")\n",
    "        \n",
    "        df_train['user_vas'] = df_train['id'].astype(str) + '_' + df_train['vas_id'].astype(str)\n",
    "\n",
    "        # Из трейна их убираем дублирующие строки по одной и той же услуге с разным таргетом\n",
    "        df_train = df_train.drop_duplicates('user_vas', keep=\"last\")\n",
    "        \n",
    "        # Функция, которая мерджит тренировочный и тестовый датасеты\n",
    "        train_res = merge_dfs(df_train, data)\n",
    "        test_res = merge_dfs(df_test, data)\n",
    "        \n",
    "        print(\"Датасет признаков присоединен к тесту и трейну\")\n",
    "        \n",
    "        train_res, med_1, med_3, med_5, med_207 = prepare_df_train(train_res)\n",
    "        test_res = prepare_df_test(test_res, med_1, med_3, med_5, med_207)\n",
    "        print(\"Датасеты подготовлены к модели\")\n",
    "        \n",
    "        feat_ok, feat_const, feat_categorical, feat_float = features_types(train_res)\n",
    "        print(\"Определены типы признаков\")\n",
    "        \n",
    "        with open(self.model_file, 'rb') as model_file:\n",
    "            model = pickle.load(model_file)\n",
    "        print(\"Модель импортирована\")\n",
    "        \n",
    "        preds = model.predict_proba(test_res)[:,1]\n",
    "        result = pd.concat([test_res['id'], test_res['vas_id'], test_res['buy_time'], pd.Series(preds)], axis=1)\n",
    "        result = result.rename(columns={0: 'target'})\n",
    "        print(\"Предсказания для теста сделаны\")\n",
    "        \n",
    "        # Сохранение в csv\n",
    "        result.to_csv('answers_test.csv', encoding=\"utf-8-sig\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "overhead-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if DataPipeline(train_csv=/Users/mac/Documents/course_megafon/course_proj/data/data_train.csv, test_csv=data_test.csv, features_csv=/Users/mac/Documents/course_megafon/course_proj/data/features.csv, model_file=/Users/mac/Documents/course_megafon/course_proj/models/xgb_estimator.pickle) is complete\n",
      "INFO: Informed scheduler that task   DataPipeline__Users_mac_Docum__Users_mac_Docum_data_test_csv_6ef31b432f   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 4391] Worker Worker(salt=824245955, workers=1, host=MacBook-Pro-di-Mac.local, username=mac, pid=4391) running   DataPipeline(train_csv=/Users/mac/Documents/course_megafon/course_proj/data/data_train.csv, test_csv=data_test.csv, features_csv=/Users/mac/Documents/course_megafon/course_proj/data/features.csv, model_file=/Users/mac/Documents/course_megafon/course_proj/models/xgb_estimator.pickle)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасеты импортированы\n",
      "Датасеты оптимизированы\n",
      "Датасет признаков присоединен к тесту и трейну\n",
      "Добавлены новые фичи\n",
      "Удалены столбцы с низкой значимостью\n",
      "Добавлены новые фичи\n",
      "Удалены столбцы с низкой значимостью\n",
      "Датасеты подготовлены к модели\n",
      "Определены типы признаков\n",
      "Модель импортирована\n",
      "Предсказания для теста сделаны\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 4391] Worker Worker(salt=824245955, workers=1, host=MacBook-Pro-di-Mac.local, username=mac, pid=4391) done      DataPipeline(train_csv=/Users/mac/Documents/course_megafon/course_proj/data/data_train.csv, test_csv=data_test.csv, features_csv=/Users/mac/Documents/course_megafon/course_proj/data/features.csv, model_file=/Users/mac/Documents/course_megafon/course_proj/models/xgb_estimator.pickle)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   DataPipeline__Users_mac_Docum__Users_mac_Docum_data_test_csv_6ef31b432f   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=824245955, workers=1, host=MacBook-Pro-di-Mac.local, username=mac, pid=4391) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 ran successfully:\n",
      "    - 1 DataPipeline(...)\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    train_csv = os.path.join(Path(os.getcwd()), \"data\", \"data_train.csv\")\n",
    "    test_csv = 'data_test.csv'\n",
    "    features_csv = os.path.join(Path(os.getcwd()), \"data\", \"features.csv\")\n",
    "    model_file = os.path.join(Path(os.getcwd()), \"models\", \"xgb_estimator.pickle\")\n",
    "\n",
    "    luigi.build([DataPipeline(train_csv, test_csv, features_csv, model_file)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-lodge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
